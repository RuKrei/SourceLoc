{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source localization pipeline - Stage1\n",
    "- freesurfer\n",
    "- .fif-File preparations\n",
    " \n",
    " \n",
    "\n",
    "\n",
    "#### Author\n",
    "Rudi Kreidenhuber <Rudi.Kreidenhuber@gmail.com>\n",
    "#### License\n",
    "BSD (3-clause)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subject selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The following patients/ subjects are available:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58e7b427aae347818f32b3ed45b80430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Subject: ', layout=Layout(width='auto'), options=('CCT KV08082000', 'FT05â€¦"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from src import SubjectDropDowner\n",
    "import os\n",
    "import glob\n",
    "import mne\n",
    "\n",
    "input_dir = \"../../MEG\"\n",
    "\n",
    "def get_subject_list(input_dir = input_dir) -> list:\n",
    "    analist = glob.glob(os.path.join(input_dir, \"*\"))\n",
    "    analist = [os.path.basename(f) for f in analist if os.path.isdir(f)]\n",
    "    fiflist = [os.path.basename(f) for f in glob.glob(os.path.join(input_dir, \"*trans_tsss.fif\"))]\n",
    "    fiflist = [f.split(\"_\")[0] for f in fiflist]\n",
    "    subjectlist = set(fiflist + analist)\n",
    "    subjectlist = sorted([f for f in subjectlist])\n",
    "    return subjectlist\n",
    "\n",
    "subjectlist = get_subject_list()\n",
    "\n",
    "dl = SubjectDropDowner.SubjectDropDowner(subjectlist)\n",
    "drop_menu = dl.create_subject_dropdown_widget()\n",
    "print(f\"\\n\\nThe following patients/ subjects are available:\")\n",
    "drop_menu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# .fif - File preprocessing\n",
    "- Files are being filtered, downsampled and concatenated\n",
    "- Files without head transposition and artifact correction via tsss are omitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subject = dl.get_subject_dropdown().value\n",
    "subject = \"sub-FT05042011\"\n",
    "\n",
    "if not subject.startswith(\"sub-\"):\n",
    "    ject = subject\n",
    "    subject = \"sub-\" + subject\n",
    "else:\n",
    "    subject = subject\n",
    "    ject = subject.replace(\"sub-\", \"\")\n",
    "\n",
    "print(f\"Ject: {ject}, Subject: {subject}\")\n",
    "\n",
    "if not os.path.isdir(os.path.join(input_dir, \"processed\")):\n",
    "    os.mkdir(os.path.join(input_dir, \"processed\"))\n",
    "\n",
    "output_dir = os.path.join(input_dir, \"processed\")\n",
    "\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "    print(f\"Created output directory: {output_dir}\")\n",
    "\n",
    "target_file = os.path.join(output_dir, str(subject + \"_prepped.fif\"))\n",
    "\n",
    "# Make sure we haven't already processed this subject\n",
    "if os.path.isfile(target_file):\n",
    "    print(f\"\\n\\nThe file {target_file} already exists, aborting.\\n\\n\")\n",
    "    raise(FileExistsError)\n",
    "\n",
    "# Configuration \n",
    "# Filter and resample\n",
    "l_freq: float = 0.1         # lower pass-band edge\n",
    "h_freq: float = 50.         # higher pass-band edge\n",
    "fir_design: str = \"firwin\"  # Filter design method\n",
    "s_freq: int = 300           # target sampling frequency\n",
    "\n",
    "# Get all .fif files of the subject\n",
    "raws = glob.glob(os.path.join(input_dir, \"*_trans_tsss.fif\"))\n",
    "raws = [f for f in raws if ject in f]\n",
    "print(f\"The following raw files were found for preprocessing:\\n{raws}\")\n",
    "\n",
    "if not raws == []:\n",
    "    prep_raws = []\n",
    "    for r in raws:\n",
    "        raw = mne.io.read_raw(r)\n",
    "        # filter\n",
    "        raw.load_data()\n",
    "        raw = raw.filter(l_freq, h_freq, fir_design=fir_design)\n",
    "        # downsample\n",
    "        if not raw.info[\"sfreq\"] == s_freq:\n",
    "            raw = raw.resample(s_freq)\n",
    "        prep_raws.append(raw)\n",
    "        del raw\n",
    "\n",
    "    # concatenate\n",
    "    try:\n",
    "        raw = mne.concatenate_raws(prep_raws)\n",
    "        raw.save(target_file, overwrite=False)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n\\nSomething went wrong, when trying to concatenate the raw files:\\n\\nError: {e}\")\n",
    "\n",
    "    del prep_raws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freesurfer and hippocampal segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dicom_file(subject) -> str:\n",
    "    if not os.path.isdir(os.path.join(input_dir, subject)):\n",
    "        print(f\"No anatomical data found for {subject}, aborting\\n\\n\")\n",
    "        raise(FileNotFoundError)\n",
    "    else:\n",
    "        dicom_path = os.path.join(input_dir, subject, \"1*\", \"100*\", \"1*\", \"*\")\n",
    "        dicom = str(glob.glob(dicom_path, recursive=True)[0])\n",
    "        dicom = os.path.abspath(dicom)\n",
    "        return dicom\n",
    "\n",
    "def path_to_wsl(file) -> str:\n",
    "    file = file.replace(\"\\\\\", \"/\")\n",
    "    return file.replace(\"c:\", \"/mnt/c\")\n",
    "\n",
    "def get_dicom_path(subject) -> str:\n",
    "    dicom = get_dicom_file(subject)\n",
    "    dicom = path_to_wsl(dicom)\n",
    "    return dicom\n",
    "\n",
    "def get_watershed_comand(subject) -> str:\n",
    "    return f'python -c \"import mne; mne.bem.make_watershed_bem(subject=\\'{subject}\\')\"'\n",
    "\n",
    "def get_recon_all_command(ject) -> str:\n",
    "    dicom = get_dicom_path(ject)\n",
    "    subject = \"sub-\" + ject\n",
    "    command = f\"recon-all -s {subject} -i {dicom} -all && segmentHA_T1.sh {subject} && {get_watershed_comand(subject)}\" \n",
    "    return command\n",
    "\n",
    "command = get_recon_all_command(ject)\n",
    "\n",
    "print(f\"\\n\\nExecute the following command in the bash shell:\\n \\\n",
    "      (This will take hours...)\\n\\n{command}\\n\\n\")\n",
    "\n",
    "print(f\"Meanwhile use brainstorm3 to mark and group spikes on the \\\n",
    "following file:\\n\\n {os.path.abspath(target_file)}\\n\\n\")\n",
    "\n",
    "event_file_name = target_file.split(\".fif\")[0] + \".txt\"\n",
    "print(f\"Save the eventfile in the same directory as the raw file as: {os.path.basename(event_file_name)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "714ec2861fe51f653782d6ca8417dafac2f9d7733f44905a1c5786f30e04b9c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}