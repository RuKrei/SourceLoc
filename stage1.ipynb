{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source localization pipeline - Stage1\n",
    "- freesurfer\n",
    "- .fif-File preparations\n",
    " \n",
    " \n",
    "\n",
    "\n",
    "#### Author\n",
    "Rudi Kreidenhuber <Rudi.Kreidenhuber@gmail.com>\n",
    "#### License\n",
    "BSD (3-clause)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subject selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from src import SubjectDropDowner\n",
    "import os\n",
    "import glob\n",
    "import mne\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "input_dir = \"../../MEG\"\n",
    "\n",
    "def get_subject_list(input_dir = input_dir) -> list:\n",
    "    analist = glob.glob(os.path.join(input_dir, \"*\"))\n",
    "    analist = [os.path.basename(f) for f in analist if os.path.isdir(f)]\n",
    "    fiflist = [os.path.basename(f) for f in glob.glob(os.path.join(input_dir, \"*trans_tsss.fif\"))]\n",
    "    fiflist = [f.split(\"_\")[0] for f in fiflist]\n",
    "    subjectlist = set(fiflist + analist)\n",
    "    subjectlist = sorted([f for f in subjectlist])\n",
    "    return subjectlist\n",
    "\n",
    "subjectlist = get_subject_list()\n",
    "\n",
    "dl = SubjectDropDowner.SubjectDropDowner(subjectlist)\n",
    "drop_menu = dl.create_subject_dropdown_widget()\n",
    "print(f\"\\n\\nThe following patients/ subjects are available:\")\n",
    "drop_menu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# .fif - File preprocessing\n",
    "- Files are being filtered, downsampled and concatenated\n",
    "- Files without head transposition and artifact correction via tsss are omitted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_eventfile(event_file):\n",
    "    \"\"\"\n",
    "    Receives a .csv or .txt files as exported i.e. via brainstorm and\n",
    "    transforms it according to mne-needs.\n",
    "\n",
    "    Returns a tuple:\n",
    "    new_eve_file, event_dict\n",
    "    --> = transformed event-file, dictionary with Eventnames\n",
    "    \"\"\"\n",
    "\n",
    "    eve = pd.read_csv(event_file, header=0)\n",
    "    le = LabelEncoder()\n",
    "    labels = eve.iloc[:,0]\n",
    "    print(f\"Labels --> {labels}\")\n",
    "    l_enc = le.fit_transform(labels)\n",
    "    l_enc = l_enc\n",
    "    new_eve_file = pd.DataFrame([eve.iloc[:,1], eve.iloc[:,0], (l_enc +1)]).T\n",
    "    new_eve_file.reset_index(drop=True, inplace = True)\n",
    "    new_eve_file.iloc[:,0] = (new_eve_file.iloc[:,0]*1000).astype(int)\n",
    "    new_eve_file.iloc[0,2] = 0  #create one pseudo-event (that is going to be dropped later for some reason)\n",
    "    new_eve_file.iloc[:,0] = new_eve_file.iloc[:,0].astype(int)\n",
    "    new_eve_file.iloc[:,1] = new_eve_file.iloc[:,1].astype(str)\n",
    "    new_eve_file.iloc[:,2] = new_eve_file.iloc[:,2].astype(int)\n",
    "    new_eve_file.iloc[:,1] = int(\"0\")\n",
    "\n",
    "    name_of_events = np.unique(eve.iloc[:,0])\n",
    "    name_of_events = np.sort(name_of_events)\n",
    "    event_dict=dict()\n",
    "    event_dict['ignore_me']=0\n",
    "    for i in range(name_of_events.size):\n",
    "        key = (name_of_events[i])\n",
    "        val = i + 1\n",
    "        event_dict[key] = val\n",
    "    return new_eve_file, event_dict\n",
    "\n",
    "def annotate_one_rawfile(raw, rawfile=None, picks=\"all\") -> mne.io.Raw:\n",
    "    \"\"\"\n",
    "    Receives a raw file, finds the corresponding event-file\n",
    "    Returns an annotated raw file\n",
    "    \"\"\"\n",
    "    eve_name = rawfile.split(\".fif\")[0] + \".csv\"\n",
    "    if not os.path.isfile(eve_name):\n",
    "        eve_name = rawfile.split(\".fif\")[0] + \".txt\"\n",
    "    if os.path.isfile(eve_name): # if fif-file matches event-file --> add events to fif-file\n",
    "        try:\n",
    "            print(f\"\\n\\nNow epoching events from {rawfile}\\n\\n\")\n",
    "            event_file, event_dict = transform_eventfile(eve_name)\n",
    "            \n",
    "            annot = mne.annotations_from_events(event_file, sfreq=raw.info[\"sfreq\"], event_desc=event_dict)\n",
    "            raw.set_annotations(annot)\n",
    "            print(f\"the annot object: {annot}\")\n",
    "            return raw\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            epochs = mne.Epochs(raw, events=event_file,\n",
    "                                event_id=event_dict, \n",
    "                                tmin=-1.5, tmax=1., \n",
    "                                baseline=(-1.5,-0.7), \n",
    "                                on_missing = \"ignore\", \n",
    "                                #picks=picks,\n",
    "                                reject=None,\n",
    "                                event_repeated=\"merge\",\n",
    "                                reject_by_annotation=False,  # nothing is dropped, but some epochs are empty\n",
    "                                )\n",
    "            #del(raw)\n",
    "            return epochs\n",
    "            \"\"\"\n",
    "        except Exception as e:\n",
    "            print(f\"failed at annotating events for: {rawfile}\\nbecause of {e}\")\n",
    "            return raw\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = dl.get_subject_dropdown().value\n",
    "\n",
    "if not subject.startswith(\"sub-\"):\n",
    "    ject = subject\n",
    "    subject = \"sub-\" + subject\n",
    "else:\n",
    "    subject = subject\n",
    "    ject = subject.replace(\"sub-\", \"\")\n",
    "\n",
    "print(f\"Ject: {ject}, Subject: {subject}\")\n",
    "\n",
    "raws = glob.glob(os.path.join(input_dir, \"*_trans_tsss.fif\"))\n",
    "raws = [f for f in raws if ject in f]\n",
    "print(f\"The following raw files were found for preprocessing:\\n{raws}\")\n",
    "\n",
    "rawfile = raws[0]\n",
    "raw = mne.io.read_raw_fif(rawfile, preload=True)\n",
    "\n",
    "annotate_one_rawfile(raw, rawfile)\n",
    "raw.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = dl.get_subject_dropdown().value\n",
    "\n",
    "if not subject.startswith(\"sub-\"):\n",
    "    ject = subject\n",
    "    subject = \"sub-\" + subject\n",
    "else:\n",
    "    subject = subject\n",
    "    ject = subject.replace(\"sub-\", \"\")\n",
    "\n",
    "print(f\"Ject: {ject}, Subject: {subject}\")\n",
    "\n",
    "if not os.path.isdir(os.path.join(input_dir, \"processed\")):\n",
    "    os.mkdir(os.path.join(input_dir, \"processed\"))\n",
    "\n",
    "output_dir = os.path.join(input_dir, \"processed\")\n",
    "\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "    print(f\"Created output directory: {output_dir}\")\n",
    "\n",
    "target_file = os.path.join(output_dir, str(subject + \"_prepped.fif\"))\n",
    "\n",
    "# Make sure we haven't already processed this subject\n",
    "if os.path.isfile(target_file):\n",
    "    print(f\"\\n\\nThe file {target_file} already exists, aborting.\\n\\n\")\n",
    "    raise(FileExistsError)\n",
    "\n",
    "# Configuration \n",
    "# Filter and resample\n",
    "l_freq: float = 0.1         # lower pass-band edge\n",
    "h_freq: float = 50.         # higher pass-band edge\n",
    "fir_design: str = \"firwin\"  # Filter design method\n",
    "s_freq: int = 300           # target sampling frequency\n",
    "\n",
    "# Get all .fif files of the subject\n",
    "raws = glob.glob(os.path.join(input_dir, \"*_trans_tsss.fif\"))\n",
    "raws = [f for f in raws if ject in f]\n",
    "print(f\"The following raw files were found for preprocessing:\\n{raws}\")\n",
    "\n",
    "if not raws == []:\n",
    "    prep_raws = []\n",
    "    for r in raws:\n",
    "        raw = mne.io.read_raw(r)\n",
    "        \n",
    "        # add events\n",
    "        \n",
    "        \n",
    "        # filter\n",
    "        raw.load_data()\n",
    "        raw = raw.filter(l_freq, h_freq, fir_design=fir_design)\n",
    "        # downsample\n",
    "        if not raw.info[\"sfreq\"] == s_freq:\n",
    "            raw = raw.resample(s_freq)\n",
    "        prep_raws.append(raw)\n",
    "        del raw\n",
    "\n",
    "    # concatenate\n",
    "    try:\n",
    "        raw = mne.concatenate_raws(prep_raws)\n",
    "        #annot = mne.Annotations([0, (raw.get_data().shape[1]-1)], [1, 1], [\"Start\", \"bad_ending\"], orig_time=raw.info[\"meas_date\"])\n",
    "        #raw.set_annotations(annot)\n",
    "        raw.save(target_file, overwrite=False)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n\\nSomething went wrong, when trying to concatenate the raw files:\\n\\nError: {e}\")\n",
    "\n",
    "    del prep_raws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freesurfer and hippocampal segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dicom_file(subject) -> str:\n",
    "    if not os.path.isdir(os.path.join(input_dir, subject)):\n",
    "        print(f\"No anatomical data found for {subject}, aborting\\n\\n\")\n",
    "        raise(FileNotFoundError)\n",
    "    else:\n",
    "        dicom_path = os.path.join(input_dir, subject, \"1*\", \"100*\", \"1*\", \"*\")\n",
    "        dicom = str(glob.glob(dicom_path, recursive=True)[0])\n",
    "        dicom = os.path.abspath(dicom)\n",
    "        return dicom\n",
    "\n",
    "def path_to_wsl(file) -> str:\n",
    "    file = file.replace(\"\\\\\", \"/\")\n",
    "    return file.replace(\"c:\", \"/mnt/c\")\n",
    "\n",
    "def get_dicom_path(subject) -> str:\n",
    "    dicom = get_dicom_file(subject)\n",
    "    dicom = path_to_wsl(dicom)\n",
    "    return dicom\n",
    "\n",
    "def get_watershed_comand(subject) -> str:\n",
    "    return f'python -c \"import mne; mne.bem.make_watershed_bem(subject=\\'{subject}\\')\"'\n",
    "\n",
    "def get_recon_all_command(ject) -> str:\n",
    "    dicom = get_dicom_path(ject)\n",
    "    subject = \"sub-\" + ject\n",
    "    command = f\"recon-all -s {subject} -i {dicom} -all && segmentHA_T1.sh {subject} && {get_watershed_comand(subject)}\" \n",
    "    return command\n",
    "\n",
    "command = get_recon_all_command(ject)\n",
    "\n",
    "print(f\"\\n\\nExecute the following command in the bash shell:\\n \\\n",
    "      (This will take hours...)\\n\\n{command}\\n\\n\")\n",
    "\n",
    "print(f\"Meanwhile use brainstorm3 to mark and group spikes on the \\\n",
    "following file:\\n\\n {os.path.abspath(target_file)}\\n\\n\")\n",
    "\n",
    "event_file_name = target_file.split(\".fif\")[0] + \".txt\"\n",
    "print(f\"Save the eventfile in the same directory as the raw file as: {os.path.basename(event_file_name)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "714ec2861fe51f653782d6ca8417dafac2f9d7733f44905a1c5786f30e04b9c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
